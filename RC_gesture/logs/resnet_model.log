class is ['01_palm', '02_l', '03_fist', '04_fist_moved', '05_thumb', '06_index', '07_ok', '08_palm_moved', '09_c', '10_down']
/home/rram/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/rram/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Conv2d: 1-1                            9,408
├─BatchNorm2d: 1-2                       128
├─ReLU: 1-3                              --
├─MaxPool2d: 1-4                         --
├─Sequential: 1-5                        --
|    └─Bottleneck: 2-1                   --
|    |    └─Conv2d: 3-1                  4,096
|    |    └─BatchNorm2d: 3-2             128
|    |    └─Conv2d: 3-3                  36,864
|    |    └─BatchNorm2d: 3-4             128
|    |    └─Conv2d: 3-5                  16,384
|    |    └─BatchNorm2d: 3-6             512
|    |    └─ReLU: 3-7                    --
|    |    └─Sequential: 3-8              16,896
|    └─Bottleneck: 2-2                   --
|    |    └─Conv2d: 3-9                  16,384
|    |    └─BatchNorm2d: 3-10            128
|    |    └─Conv2d: 3-11                 36,864
|    |    └─BatchNorm2d: 3-12            128
|    |    └─Conv2d: 3-13                 16,384
|    |    └─BatchNorm2d: 3-14            512
|    |    └─ReLU: 3-15                   --
|    └─Bottleneck: 2-3                   --
|    |    └─Conv2d: 3-16                 16,384
|    |    └─BatchNorm2d: 3-17            128
|    |    └─Conv2d: 3-18                 36,864
|    |    └─BatchNorm2d: 3-19            128
|    |    └─Conv2d: 3-20                 16,384
|    |    └─BatchNorm2d: 3-21            512
|    |    └─ReLU: 3-22                   --
├─Sequential: 1-6                        --
|    └─Bottleneck: 2-4                   --
|    |    └─Conv2d: 3-23                 32,768
|    |    └─BatchNorm2d: 3-24            256
|    |    └─Conv2d: 3-25                 147,456
|    |    └─BatchNorm2d: 3-26            256
|    |    └─Conv2d: 3-27                 65,536
|    |    └─BatchNorm2d: 3-28            1,024
|    |    └─ReLU: 3-29                   --
|    |    └─Sequential: 3-30             132,096
|    └─Bottleneck: 2-5                   --
|    |    └─Conv2d: 3-31                 65,536
|    |    └─BatchNorm2d: 3-32            256
|    |    └─Conv2d: 3-33                 147,456
|    |    └─BatchNorm2d: 3-34            256
|    |    └─Conv2d: 3-35                 65,536
|    |    └─BatchNorm2d: 3-36            1,024
|    |    └─ReLU: 3-37                   --
|    └─Bottleneck: 2-6                   --
|    |    └─Conv2d: 3-38                 65,536
|    |    └─BatchNorm2d: 3-39            256
|    |    └─Conv2d: 3-40                 147,456
|    |    └─BatchNorm2d: 3-41            256
|    |    └─Conv2d: 3-42                 65,536
|    |    └─BatchNorm2d: 3-43            1,024
|    |    └─ReLU: 3-44                   --
|    └─Bottleneck: 2-7                   --
|    |    └─Conv2d: 3-45                 65,536
|    |    └─BatchNorm2d: 3-46            256
|    |    └─Conv2d: 3-47                 147,456
|    |    └─BatchNorm2d: 3-48            256
|    |    └─Conv2d: 3-49                 65,536
|    |    └─BatchNorm2d: 3-50            1,024
|    |    └─ReLU: 3-51                   --
├─Sequential: 1-7                        --
|    └─Bottleneck: 2-8                   --
|    |    └─Conv2d: 3-52                 131,072
|    |    └─BatchNorm2d: 3-53            512
|    |    └─Conv2d: 3-54                 589,824
|    |    └─BatchNorm2d: 3-55            512
|    |    └─Conv2d: 3-56                 262,144
|    |    └─BatchNorm2d: 3-57            2,048
|    |    └─ReLU: 3-58                   --
|    |    └─Sequential: 3-59             526,336
|    └─Bottleneck: 2-9                   --
|    |    └─Conv2d: 3-60                 262,144
|    |    └─BatchNorm2d: 3-61            512
|    |    └─Conv2d: 3-62                 589,824
|    |    └─BatchNorm2d: 3-63            512
|    |    └─Conv2d: 3-64                 262,144
|    |    └─BatchNorm2d: 3-65            2,048
|    |    └─ReLU: 3-66                   --
|    └─Bottleneck: 2-10                  --
|    |    └─Conv2d: 3-67                 262,144
|    |    └─BatchNorm2d: 3-68            512
|    |    └─Conv2d: 3-69                 589,824
|    |    └─BatchNorm2d: 3-70            512
|    |    └─Conv2d: 3-71                 262,144
|    |    └─BatchNorm2d: 3-72            2,048
|    |    └─ReLU: 3-73                   --
|    └─Bottleneck: 2-11                  --
|    |    └─Conv2d: 3-74                 262,144
|    |    └─BatchNorm2d: 3-75            512
|    |    └─Conv2d: 3-76                 589,824
|    |    └─BatchNorm2d: 3-77            512
|    |    └─Conv2d: 3-78                 262,144
|    |    └─BatchNorm2d: 3-79            2,048
|    |    └─ReLU: 3-80                   --
|    └─Bottleneck: 2-12                  --
|    |    └─Conv2d: 3-81                 262,144
|    |    └─BatchNorm2d: 3-82            512
|    |    └─Conv2d: 3-83                 589,824
|    |    └─BatchNorm2d: 3-84            512
|    |    └─Conv2d: 3-85                 262,144
|    |    └─BatchNorm2d: 3-86            2,048
|    |    └─ReLU: 3-87                   --
|    └─Bottleneck: 2-13                  --
|    |    └─Conv2d: 3-88                 262,144
|    |    └─BatchNorm2d: 3-89            512
|    |    └─Conv2d: 3-90                 589,824
|    |    └─BatchNorm2d: 3-91            512
|    |    └─Conv2d: 3-92                 262,144
|    |    └─BatchNorm2d: 3-93            2,048
|    |    └─ReLU: 3-94                   --
├─Sequential: 1-8                        --
|    └─Bottleneck: 2-14                  --
|    |    └─Conv2d: 3-95                 524,288
|    |    └─BatchNorm2d: 3-96            1,024
|    |    └─Conv2d: 3-97                 2,359,296
|    |    └─BatchNorm2d: 3-98            1,024
|    |    └─Conv2d: 3-99                 1,048,576
|    |    └─BatchNorm2d: 3-100           4,096
|    |    └─ReLU: 3-101                  --
|    |    └─Sequential: 3-102            2,101,248
|    └─Bottleneck: 2-15                  --
|    |    └─Conv2d: 3-103                1,048,576
|    |    └─BatchNorm2d: 3-104           1,024
|    |    └─Conv2d: 3-105                2,359,296
|    |    └─BatchNorm2d: 3-106           1,024
|    |    └─Conv2d: 3-107                1,048,576
|    |    └─BatchNorm2d: 3-108           4,096
|    |    └─ReLU: 3-109                  --
|    └─Bottleneck: 2-16                  --
|    |    └─Conv2d: 3-110                1,048,576
|    |    └─BatchNorm2d: 3-111           1,024
|    |    └─Conv2d: 3-112                2,359,296
|    |    └─BatchNorm2d: 3-113           1,024
|    |    └─Conv2d: 3-114                1,048,576
|    |    └─BatchNorm2d: 3-115           4,096
|    |    └─ReLU: 3-116                  --
├─AdaptiveAvgPool2d: 1-9                 --
├─Linear: 1-10                           20,490
=================================================================
Total params: 23,528,522
Trainable params: 23,528,522
Non-trainable params: 0
=================================================================
Epoch [1/100]: Train loss 0.199, Train acc 93.856, Val loss 0.077, Val acc 96.800
Epoch [2/100]: Train loss 0.051, Train acc 98.488, Val loss 0.014, Val acc 99.700
Epoch [3/100]: Train loss 0.028, Train acc 99.269, Val loss 0.059, Val acc 98.550
Epoch [4/100]: Train loss 0.036, Train acc 98.994, Val loss 0.024, Val acc 99.325
Epoch [5/100]: Train loss 0.035, Train acc 98.981, Val loss 0.075, Val acc 97.375
Epoch [6/100]: Train loss 0.029, Train acc 99.131, Val loss 0.804, Val acc 87.850
Epoch [7/100]: Train loss 0.018, Train acc 99.494, Val loss 0.009, Val acc 99.850
Epoch [8/100]: Train loss 0.008, Train acc 99.794, Val loss 0.011, Val acc 99.800
Epoch [9/100]: Train loss 0.026, Train acc 99.288, Val loss 0.172, Val acc 95.350
Epoch [10/100]: Train loss 0.014, Train acc 99.631, Val loss 0.003, Val acc 99.925
Epoch [11/100]: Train loss 0.021, Train acc 99.363, Val loss 0.188, Val acc 96.200
Epoch [12/100]: Train loss 0.031, Train acc 99.106, Val loss 0.006, Val acc 99.800
Epoch [13/100]: Train loss 0.024, Train acc 99.413, Val loss 0.003, Val acc 99.925
Epoch [14/100]: Train loss 0.006, Train acc 99.844, Val loss 0.112, Val acc 97.250
Epoch [15/100]: Train loss 0.016, Train acc 99.606, Val loss 0.016, Val acc 99.625
Epoch [16/100]: Train loss 0.014, Train acc 99.675, Val loss 0.017, Val acc 99.675
Epoch [17/100]: Train loss 0.010, Train acc 99.731, Val loss 0.009, Val acc 99.875
Epoch [18/100]: Train loss 0.012, Train acc 99.681, Val loss 0.013, Val acc 99.850
Epoch [19/100]: Train loss 0.011, Train acc 99.675, Val loss 0.006, Val acc 99.925
Epoch [20/100]: Train loss 0.014, Train acc 99.625, Val loss 0.002, Val acc 99.925
Epoch [21/100]: Train loss 0.016, Train acc 99.562, Val loss 0.009, Val acc 99.850
Epoch [22/100]: Train loss 0.019, Train acc 99.469, Val loss 0.009, Val acc 99.725
Epoch [23/100]: Train loss 0.006, Train acc 99.856, Val loss 0.001, Val acc 99.975
Epoch [24/100]: Train loss 0.003, Train acc 99.925, Val loss 0.003, Val acc 99.950
Epoch [25/100]: Train loss 0.007, Train acc 99.819, Val loss 0.002, Val acc 99.925
Epoch [26/100]: Train loss 0.001, Train acc 99.969, Val loss 0.004, Val acc 99.925
Epoch [27/100]: Train loss 0.002, Train acc 99.956, Val loss 0.000, Val acc 99.975
Epoch [28/100]: Train loss 0.013, Train acc 99.594, Val loss 0.003, Val acc 99.950
Epoch [29/100]: Train loss 0.009, Train acc 99.725, Val loss 0.003, Val acc 99.925
Epoch [30/100]: Train loss 0.016, Train acc 99.600, Val loss 0.005, Val acc 99.950
Epoch [31/100]: Train loss 0.005, Train acc 99.844, Val loss 0.005, Val acc 99.925
Epoch [32/100]: Train loss 0.002, Train acc 99.938, Val loss 0.004, Val acc 99.900
Epoch [33/100]: Train loss 0.007, Train acc 99.838, Val loss 0.007, Val acc 99.700
Epoch [34/100]: Train loss 0.017, Train acc 99.531, Val loss 0.003, Val acc 99.950
Epoch [35/100]: Train loss 0.010, Train acc 99.719, Val loss 0.006, Val acc 99.850
Epoch [36/100]: Train loss 0.003, Train acc 99.919, Val loss 0.008, Val acc 99.925
Epoch [37/100]: Train loss 0.004, Train acc 99.875, Val loss 0.005, Val acc 99.950
Epoch [38/100]: Train loss 0.001, Train acc 99.969, Val loss 0.007, Val acc 99.900
Epoch [39/100]: Train loss 0.015, Train acc 99.550, Val loss 0.008, Val acc 99.775
Epoch [40/100]: Train loss 0.005, Train acc 99.856, Val loss 0.003, Val acc 99.950
Epoch [41/100]: Train loss 0.009, Train acc 99.744, Val loss 0.001, Val acc 99.975
Epoch [42/100]: Train loss 0.001, Train acc 99.988, Val loss 0.001, Val acc 99.975
Epoch [43/100]: Train loss 0.003, Train acc 99.913, Val loss 0.006, Val acc 99.925
Epoch [44/100]: Train loss 0.010, Train acc 99.763, Val loss 0.003, Val acc 99.925
Epoch [45/100]: Train loss 0.001, Train acc 99.969, Val loss 0.002, Val acc 99.950
Epoch [46/100]: Train loss 0.001, Train acc 99.981, Val loss 0.003, Val acc 99.950
Epoch [47/100]: Train loss 0.000, Train acc 100.000, Val loss 0.004, Val acc 99.950
Epoch [48/100]: Train loss 0.002, Train acc 99.944, Val loss 0.041, Val acc 98.700
Epoch [49/100]: Train loss 0.003, Train acc 99.894, Val loss 0.001, Val acc 99.975
Epoch [50/100]: Train loss 0.023, Train acc 99.444, Val loss 0.005, Val acc 99.925
Epoch [51/100]: Train loss 0.007, Train acc 99.838, Val loss 0.005, Val acc 99.875
Epoch [52/100]: Train loss 0.005, Train acc 99.856, Val loss 0.001, Val acc 100.000
Epoch [53/100]: Train loss 0.002, Train acc 99.944, Val loss 0.001, Val acc 100.000
Epoch [54/100]: Train loss 0.001, Train acc 99.988, Val loss 0.004, Val acc 99.875
Epoch [55/100]: Train loss 0.001, Train acc 99.981, Val loss 0.002, Val acc 99.950
Epoch [56/100]: Train loss 0.000, Train acc 99.994, Val loss 0.004, Val acc 99.925
Epoch [57/100]: Train loss 0.001, Train acc 99.975, Val loss 0.002, Val acc 99.950
Epoch [58/100]: Train loss 0.000, Train acc 100.000, Val loss 0.003, Val acc 99.950
Epoch [59/100]: Train loss 0.000, Train acc 100.000, Val loss 0.002, Val acc 99.950
Epoch [60/100]: Train loss 0.013, Train acc 99.663, Val loss 0.075, Val acc 97.275
Epoch [61/100]: Train loss 0.012, Train acc 99.706, Val loss 0.009, Val acc 99.825
Epoch [62/100]: Train loss 0.004, Train acc 99.894, Val loss 0.001, Val acc 99.950
Epoch [63/100]: Train loss 0.004, Train acc 99.888, Val loss 0.007, Val acc 99.825
Epoch [64/100]: Train loss 0.001, Train acc 99.988, Val loss 0.003, Val acc 99.875
Epoch [65/100]: Train loss 0.000, Train acc 99.988, Val loss 0.001, Val acc 99.950
Epoch [66/100]: Train loss 0.001, Train acc 99.975, Val loss 0.032, Val acc 98.975
Epoch [67/100]: Train loss 0.006, Train acc 99.813, Val loss 0.001, Val acc 99.950
Epoch [68/100]: Train loss 0.003, Train acc 99.950, Val loss 0.005, Val acc 99.875
Epoch [69/100]: Train loss 0.008, Train acc 99.856, Val loss 0.007, Val acc 99.825
Epoch [70/100]: Train loss 0.004, Train acc 99.888, Val loss 0.009, Val acc 99.950
Epoch [71/100]: Train loss 0.003, Train acc 99.919, Val loss 0.007, Val acc 99.925
Epoch [72/100]: Train loss 0.001, Train acc 99.981, Val loss 0.005, Val acc 99.925
Epoch [73/100]: Train loss 0.000, Train acc 100.000, Val loss 0.004, Val acc 99.950
Epoch [74/100]: Train loss 0.001, Train acc 99.994, Val loss 0.006, Val acc 99.925
Epoch [75/100]: Train loss 0.000, Train acc 99.988, Val loss 0.008, Val acc 99.875
Epoch [76/100]: Train loss 0.021, Train acc 99.475, Val loss 0.005, Val acc 99.900
Epoch [77/100]: Train loss 0.002, Train acc 99.956, Val loss 0.002, Val acc 99.950
Epoch [78/100]: Train loss 0.001, Train acc 99.956, Val loss 0.005, Val acc 99.925
Epoch [79/100]: Train loss 0.003, Train acc 99.931, Val loss 0.004, Val acc 99.875
Epoch [80/100]: Train loss 0.001, Train acc 99.981, Val loss 0.002, Val acc 99.975
Epoch [81/100]: Train loss 0.000, Train acc 100.000, Val loss 0.003, Val acc 99.925
Epoch [82/100]: Train loss 0.000, Train acc 99.988, Val loss 0.006, Val acc 99.925
Epoch [83/100]: Train loss 0.006, Train acc 99.850, Val loss 0.009, Val acc 99.875
Epoch [84/100]: Train loss 0.004, Train acc 99.900, Val loss 0.003, Val acc 99.950
Epoch [85/100]: Train loss 0.001, Train acc 99.988, Val loss 0.000, Val acc 99.975
Epoch [86/100]: Train loss 0.000, Train acc 100.000, Val loss 0.000, Val acc 99.975
Epoch [87/100]: Train loss 0.000, Train acc 99.981, Val loss 0.001, Val acc 99.975
Epoch [88/100]: Train loss 0.003, Train acc 99.906, Val loss 0.011, Val acc 99.725
Epoch [89/100]: Train loss 0.004, Train acc 99.900, Val loss 0.003, Val acc 99.975
Epoch [90/100]: Train loss 0.001, Train acc 99.963, Val loss 0.003, Val acc 99.925
Epoch [91/100]: Train loss 0.000, Train acc 99.994, Val loss 0.002, Val acc 99.950
Epoch [92/100]: Train loss 0.008, Train acc 99.819, Val loss 0.053, Val acc 98.675
Epoch [93/100]: Train loss 0.002, Train acc 99.938, Val loss 0.006, Val acc 99.825
Epoch [94/100]: Train loss 0.008, Train acc 99.794, Val loss 0.005, Val acc 99.950
Epoch [95/100]: Train loss 0.001, Train acc 99.981, Val loss 0.006, Val acc 99.925
Epoch [96/100]: Train loss 0.002, Train acc 99.944, Val loss 0.007, Val acc 99.900
Epoch [97/100]: Train loss 0.000, Train acc 100.000, Val loss 0.009, Val acc 99.925
Epoch [98/100]: Train loss 0.004, Train acc 99.825, Val loss 0.006, Val acc 99.925
Epoch [99/100]: Train loss 0.001, Train acc 99.969, Val loss 0.003, Val acc 99.950
Epoch [100/100]: Train loss 0.001, Train acc 99.981, Val loss 0.001, Val acc 99.925
